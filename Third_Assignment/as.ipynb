{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01a00830",
   "metadata": {},
   "source": [
    "# Image Colorization Using Deep Convolutional Neural Networks\n",
    "## BBM 418 - Assignment 3\n",
    "\n",
    "**Student Name:** Ahmet Oğuzhan Kökülü  \n",
    "**Student Number:** b2220356053\n",
    "\n",
    "**Demo Video Link:** (sonradan ekleyecem)\n",
    "\n",
    "**Dataset Drive Link:** https://drive.google.com/drive/folders/12lfa_UkMO9aBJGw989s1h1UDF-NbrDa9?usp=sharing\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "This notebook implements an image colorization system using Deep Convolutional Neural Networks (DCNNs) with an encoder-decoder architecture. The model takes grayscale images (L channel from L*a*b* color space) as input and predicts the corresponding color channels (a* and b*)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2890a8ae",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03f25d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory allocated: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from skimage import color, io\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"GPU memory allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f343038",
   "metadata": {},
   "source": [
    "## 2. Introduction\n",
    "\n",
    "Image colorization is the process of adding realistic colors to grayscale images.\n",
    "\n",
    "In this assignment, we use an encoder-decoder architecture with Deep Convolutional Neural Networks (DCNNs) to learn the mapping between grayscale images and their colorized versions.\n",
    "\n",
    "### L*a*b* Color Space\n",
    "\n",
    "We use the L*a*b* color space (CIELAB) which has three components:\n",
    "- **L***: Lightness (0 for black to 100 for white)\n",
    "- **a***: Green-to-red spectrum\n",
    "- **b***: Blue-to-yellow spectrum\n",
    "\n",
    "This color space is ideal for colorization because it separates brightness (L*) from color information (a*, b*). We use:\n",
    "- **Input**: L channel (grayscale image)\n",
    "- **Target**: a and b channels (color information)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65417023",
   "metadata": {},
   "source": [
    "## 3. Dataset Preparation\n",
    "\n",
    "### 3.1 Configuration and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa01724a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model path: ./models\n",
      "Image size: 256x256\n",
      "Batch size: 1\n",
      "Training split: 80.0%\n"
     ]
    }
   ],
   "source": [
    "# Local dataset configuration\n",
    "DATASET_PATH = './dataset'  # Local path to dataset folder\n",
    "OUTPUT_PATH = './outputs'  # Folder for saving outputs (images, plots)\n",
    "MODEL_PATH = './models'    # Folder for saving trained models\n",
    "\n",
    "# Create directories if they don't exist\n",
    "import os\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "\n",
    "# Image configuration\n",
    "IMG_SIZE = 256  # Architecture is hardcoded for 256x256\n",
    "BATCH_SIZE = 1  # Batch size of 1 for 6GB GPU (only option that works)\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "# Training configuration\n",
    "NUM_EPOCHS = 30\n",
    "LEARNING_RATE = 0.001\n",
    "TRAIN_SPLIT = 0.8\n",
    "\n",
    "print(f\"Model path: {MODEL_PATH}\")\n",
    "print(f\"Image size: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "\n",
    "\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Training split: {TRAIN_SPLIT * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49de4144",
   "metadata": {},
   "source": [
    "### 3.2 Prepare Dataset Split\n",
    "\n",
    "This function creates train/test split indices from dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c43c052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning images from: ./dataset\n",
      "Found 5000 color images\n",
      "\n",
      "Dataset split:\n",
      "  Training images: 4000\n",
      "  Test images: 1000\n",
      "  Train/Test ratio: 0.8/0.19999999999999996\n"
     ]
    }
   ],
   "source": [
    "def prepare_dataset_split(color_images_dir, train_split=0.8):\n",
    "    \"\"\"\n",
    "    Prepare train/test split from color images in local directory.\n",
    "    Returns lists of image file paths.\n",
    "    \n",
    "    Args:\n",
    "        color_images_dir: Directory containing color images locally\n",
    "        train_split: Ratio of training images\n",
    "    \n",
    "    Returns:\n",
    "        train_files: List of training image paths\n",
    "        test_files: List of test image paths\n",
    "    \"\"\"\n",
    "    print(f\"Scanning images from: {color_images_dir}\")\n",
    "    \n",
    "    # Get all image files\n",
    "    image_files = []\n",
    "    for ext in ['*.jpg', '*.jpeg', '*.png', '*.JPG', '*.JPEG', '*.PNG']:\n",
    "        image_files.extend(list(Path(color_images_dir).glob(ext)))\n",
    "    \n",
    "    print(f\"Found {len(image_files)} color images\")\n",
    "    \n",
    "    if len(image_files) == 0:\n",
    "        raise ValueError(f\"No images found in {color_images_dir}. Please check the path.\")\n",
    "    \n",
    "    # Shuffle with fixed seed for reproducibility\n",
    "    random.seed(SEED)\n",
    "    random.shuffle(image_files)\n",
    "    \n",
    "    # Split into train and test\n",
    "    split_idx = int(len(image_files) * train_split)\n",
    "    train_files = image_files[:split_idx]\n",
    "    test_files = image_files[split_idx:]\n",
    "    \n",
    "    print(f\"\\nDataset split:\")\n",
    "    print(f\"  Training images: {len(train_files)}\")\n",
    "    print(f\"  Test images: {len(test_files)}\")\n",
    "    print(f\"  Train/Test ratio: {train_split}/{1-train_split}\")\n",
    "    \n",
    "    return train_files, test_files\n",
    "\n",
    "# Prepare the dataset split from local folder\n",
    "train_image_files, test_image_files = prepare_dataset_split(DATASET_PATH, TRAIN_SPLIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260aec3d",
   "metadata": {},
   "source": [
    "### 3.3 Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a70248b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for image colorization.\n",
    "    Loads color images directly from Google Drive and converts to L*a*b* on-the-fly.\n",
    "    Returns L channel as input and ab channels as target.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_files, img_size=256, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_files: List of Path objects pointing to color images in Drive\n",
    "            img_size: Size to resize images to\n",
    "            transform: Optional transforms to apply\n",
    "        \"\"\"\n",
    "        self.image_files = image_files\n",
    "        self.img_size = img_size\n",
    "        self.transform = transform\n",
    "        \n",
    "        print(f\"Initialized dataset with {len(self.image_files)} images\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load color image directly from Drive\n",
    "        img_path = str(self.image_files[idx])\n",
    "        \n",
    "        try:\n",
    "            # Read RGB image\n",
    "            img_rgb = io.imread(img_path)\n",
    "            \n",
    "            # Convert to RGB if grayscale or RGBA\n",
    "            if len(img_rgb.shape) == 2:\n",
    "                img_rgb = np.stack([img_rgb] * 3, axis=-1)\n",
    "            elif img_rgb.shape[2] == 4:\n",
    "                img_rgb = img_rgb[:, :, :3]\n",
    "            \n",
    "            # Resize\n",
    "            img_pil = Image.fromarray(img_rgb)\n",
    "            img_pil = img_pil.resize((self.img_size, self.img_size), Image.LANCZOS)\n",
    "            img_rgb = np.array(img_pil)\n",
    "            \n",
    "            # Convert RGB to L*a*b* color space\n",
    "            img_lab = color.rgb2lab(img_rgb).astype(np.float32)\n",
    "            \n",
    "            # Split into L and ab channels\n",
    "            L = img_lab[:, :, 0]  # Lightness channel\n",
    "            ab = img_lab[:, :, 1:]  # Color channels (a*, b*)\n",
    "            \n",
    "            # Normalize L to [0, 1] (L is in range [0, 100])\n",
    "            L = L / 100.0\n",
    "            \n",
    "            # Normalize ab to [-1, 1] (ab is approximately in range [-128, 127])\n",
    "            ab = ab / 128.0\n",
    "            \n",
    "            # Convert to tensors\n",
    "            L = torch.FloatTensor(L).unsqueeze(0)  # Shape: (1, H, W)\n",
    "            ab = torch.FloatTensor(ab).permute(2, 0, 1)  # Shape: (2, H, W)\n",
    "            \n",
    "            if self.transform:\n",
    "                # Apply same transform to both L and ab\n",
    "                seed = np.random.randint(2147483647)\n",
    "                random.seed(seed)\n",
    "                torch.manual_seed(seed)\n",
    "                L = self.transform(L)\n",
    "                random.seed(seed)\n",
    "                torch.manual_seed(seed)\n",
    "                ab = self.transform(ab)\n",
    "            \n",
    "            return L, ab, img_lab  # Return original lab_img for visualization\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            # Return a black image as fallback\n",
    "            L = torch.zeros(1, self.img_size, self.img_size)\n",
    "            ab = torch.zeros(2, self.img_size, self.img_size)\n",
    "            lab_img = np.zeros((self.img_size, self.img_size, 3), dtype=np.float32)\n",
    "            return L, ab, lab_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bfa137",
   "metadata": {},
   "source": [
    "### 3.4 Create Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b739099f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized dataset with 4000 images\n",
      "Initialized dataset with 1000 images\n",
      "Training set: 4000 images, 4000 batches\n",
      "Validation/Test set: 1000 images, 1000 batches\n"
     ]
    }
   ],
   "source": [
    "# Create datasets from Drive image files (uncomment when Drive is mounted and split is prepared)\n",
    "train_dataset = ColorizationDataset(train_image_files, img_size=IMG_SIZE)\n",
    "test_dataset = ColorizationDataset(test_image_files, img_size=IMG_SIZE)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                         num_workers=NUM_WORKERS, pin_memory=True)\n",
    "# Using test_loader as val_loader (validation) since assignment uses train/test split\n",
    "val_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                        num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_loader = val_loader  # Alias for compatibility\n",
    "\n",
    "print(f\"Training set: {len(train_dataset)} images, {len(train_loader)} batches\")\n",
    "print(f\"Validation/Test set: {len(test_dataset)} images, {len(val_loader)} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a04f4e0",
   "metadata": {},
   "source": [
    "### 3.5 Visualize Sample Images and L*a*b* Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34fff82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_lab_channels(dataset, num_samples=3):\n",
    "    \"\"\"Visualize L, a, b channels separately for sample images.\"\"\"\n",
    "    fig, axes = plt.subplots(num_samples, 5, figsize=(20, 4 * num_samples))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        L, ab, lab_img = dataset[i]\n",
    "        \n",
    "        # Convert back from tensors\n",
    "        L_img = (L.squeeze().numpy() * 100.0).astype(np.float32)\n",
    "        ab_img = (ab.permute(1, 2, 0).numpy() * 128.0).astype(np.float32)\n",
    "        \n",
    "        # Reconstruct full LAB image\n",
    "        lab_reconstructed = np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.float32)\n",
    "        lab_reconstructed[:, :, 0] = L_img\n",
    "        lab_reconstructed[:, :, 1:] = ab_img\n",
    "        \n",
    "        # Convert to RGB\n",
    "        rgb_img = color.lab2rgb(lab_reconstructed)\n",
    "        \n",
    "        # Plot\n",
    "        axes[i, 0].imshow(L_img, cmap='gray')\n",
    "        axes[i, 0].set_title(f'Sample {i+1}: L (Lightness)')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        axes[i, 1].imshow(ab_img[:, :, 0], cmap='RdYlGn_r')\n",
    "        axes[i, 1].set_title('a* (Green-Red)')\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        axes[i, 2].imshow(ab_img[:, :, 1], cmap='YlGnBu_r')\n",
    "        axes[i, 2].set_title('b* (Blue-Yellow)')\n",
    "        axes[i, 2].axis('off')\n",
    "        \n",
    "        axes[i, 3].imshow(rgb_img)\n",
    "        axes[i, 3].set_title('RGB (Ground Truth)')\n",
    "        axes[i, 3].axis('off')\n",
    "        \n",
    "        # Show grayscale version\n",
    "        axes[i, 4].imshow(L_img, cmap='gray')\n",
    "        axes[i, 4].set_title('Grayscale Input')\n",
    "        axes[i, 4].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# visualize_lab_channels(train_dataset, num_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3997e349",
   "metadata": {},
   "source": [
    "## 4. Model Architecture\n",
    "\n",
    "### 4.1 Low-Level Feature Extraction Network\n",
    "\n",
    "This network extracts local spatial features from the input grayscale image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2318f85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LowLevelFeatureExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    Low-level feature extraction network.\n",
    "    Follows the architecture from Figure 3:\n",
    "    Conv1-6 with progressive downsampling from 256x256 -> 128x128 -> 64x64 -> 32x32\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(LowLevelFeatureExtractor, self).__init__()\n",
    "        \n",
    "        # Conv1-2: 256x256 -> 128x128\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1)  # 256->128\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Conv3-4: 128x128 -> 64x64\n",
    "        self.conv3 = nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1)  # 128->64\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        # Conv5-6: 64x64 -> 32x32\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1)  # 64->32\n",
    "        self.bn5 = nn.BatchNorm2d(256)\n",
    "        self.conv6 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn6 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input: (B, 1, 256, 256)\n",
    "        x = self.relu(self.bn1(self.conv1(x)))  # (B, 64, 128, 128)\n",
    "        x = self.relu(self.bn2(self.conv2(x)))  # (B, 128, 128, 128)\n",
    "        \n",
    "        x = self.relu(self.bn3(self.conv3(x)))  # (B, 128, 64, 64)\n",
    "        x = self.relu(self.bn4(self.conv4(x)))  # (B, 256, 64, 64)\n",
    "        \n",
    "        x = self.relu(self.bn5(self.conv5(x)))  # (B, 256, 32, 32)\n",
    "        x = self.relu(self.bn6(self.conv6(x)))  # (B, 512, 32, 32)\n",
    "        \n",
    "        return x  # Output: (B, 512, 32, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cc2d62",
   "metadata": {},
   "source": [
    "### 4.2 Global Feature Extraction Network (Baseline)\n",
    "\n",
    "This network extracts global context features. We'll start with a simple baseline, then integrate pretrained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f423108b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalFeatureExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    Global feature extraction network (baseline).\n",
    "    Extracts global context with Conv7-8 to produce a 1000-dimensional feature vector.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(GlobalFeatureExtractor, self).__init__()\n",
    "        \n",
    "        # Conv7-8: Further process features\n",
    "        self.conv7 = nn.Conv2d(1, 512, kernel_size=3, stride=2, padding=1)  # Assuming input from original image\n",
    "        self.bn7 = nn.BatchNorm2d(512)\n",
    "        self.conv8 = nn.Conv2d(512, 256, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn8 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Fully connected layers to produce 1000-dim feature vector\n",
    "        self.fc1 = nn.Linear(256, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 1000)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input: (B, 1, H, W) - original grayscale image\n",
    "        x = self.relu(self.bn7(self.conv7(x)))\n",
    "        x = self.relu(self.bn8(self.conv8(x)))\n",
    "        \n",
    "        x = self.pool(x)  # (B, 256, 1, 1)\n",
    "        x = x.view(x.size(0), -1)  # (B, 256)\n",
    "        \n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)  # (B, 1000)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b7ec9a",
   "metadata": {},
   "source": [
    "### 4.3 Fusion Block\n",
    "\n",
    "Combines low-level features with global features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2235a88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Fusion block that combines low-level features (B, 512, 32, 32) \n",
    "    with global features (B, 1000) into a fused representation (B, 256, 32, 32).\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(FusionBlock, self).__init__()\n",
    "        \n",
    "        # Project global features to spatial dimensions\n",
    "        self.fc = nn.Linear(1000, 256 * 32 * 32)\n",
    "        \n",
    "        # Combine low-level (512 channels) + global (256 channels) = 768 channels\n",
    "        self.conv_fuse = nn.Conv2d(512 + 256, 256, kernel_size=1)\n",
    "        self.bn = nn.BatchNorm2d(256)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, low_level_features, global_features):\n",
    "        # low_level_features: (B, 512, 32, 32)\n",
    "        # global_features: (B, 1000)\n",
    "        \n",
    "        batch_size = low_level_features.size(0)\n",
    "        \n",
    "        # Project global features to spatial map\n",
    "        global_spatial = self.fc(global_features)  # (B, 256*32*32)\n",
    "        global_spatial = global_spatial.view(batch_size, 256, 32, 32)  # (B, 256, 32, 32)\n",
    "        \n",
    "        # Concatenate along channel dimension\n",
    "        fused = torch.cat([low_level_features, global_spatial], dim=1)  # (B, 768, 32, 32)\n",
    "        \n",
    "        # Reduce channels\n",
    "        fused = self.relu(self.bn(self.conv_fuse(fused)))  # (B, 256, 32, 32)\n",
    "        \n",
    "        return fused"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f10cad",
   "metadata": {},
   "source": [
    "### 4.4 Decoder Network\n",
    "\n",
    "The decoder progressively upsamples the fused features to predict the ab channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc1f8fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder network that upsamples fused features from 32x32 to 256x256\n",
    "    and predicts the ab channels.\n",
    "    Follows the architecture from Figure 3: 32x32 -> 64x64 -> 128x128 -> 256x256\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # Upsampling path\n",
    "        # 32x32 -> 64x64\n",
    "        self.upconv1 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.conv1 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn1_2 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # 64x64 -> 128x128\n",
    "        self.upconv2 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn2_2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # 128x128 -> 256x256\n",
    "        self.upconv3 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.bn3_2 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        # Final convolution to produce 2 channels (ab)\n",
    "        self.final_conv = nn.Conv2d(32, 2, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.tanh = nn.Tanh()  # Output in range [-1, 1]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input: (B, 256, 32, 32)\n",
    "        \n",
    "        # 32x32 -> 64x64\n",
    "        x = self.relu(self.bn1(self.upconv1(x)))  # (B, 128, 64, 64)\n",
    "        x = self.relu(self.bn1_2(self.conv1(x)))\n",
    "        \n",
    "        # 64x64 -> 128x128\n",
    "        x = self.relu(self.bn2(self.upconv2(x)))  # (B, 64, 128, 128)\n",
    "        x = self.relu(self.bn2_2(self.conv2(x)))\n",
    "        \n",
    "        # 128x128 -> 256x256\n",
    "        x = self.relu(self.bn3(self.upconv3(x)))  # (B, 32, 256, 256)\n",
    "        x = self.relu(self.bn3_2(self.conv3(x)))\n",
    "        \n",
    "        # Final output\n",
    "        x = self.tanh(self.final_conv(x))  # (B, 2, 256, 256)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d411c6cc",
   "metadata": {},
   "source": [
    "### 4.5 Complete Baseline Colorization Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0179ec75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizationModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete colorization model combining all components.\n",
    "    \"\"\"\n",
    "    def __init__(self, use_pretrained_global=None):\n",
    "        super(ColorizationModel, self).__init__()\n",
    "        \n",
    "        self.low_level_extractor = LowLevelFeatureExtractor()\n",
    "        \n",
    "        if use_pretrained_global is None:\n",
    "            # Use baseline global feature extractor\n",
    "            self.global_extractor = GlobalFeatureExtractor()\n",
    "        else:\n",
    "            # Use pretrained model (to be implemented)\n",
    "            self.global_extractor = use_pretrained_global\n",
    "        \n",
    "        self.fusion = FusionBlock()\n",
    "        self.decoder = Decoder()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (B, 1, 256, 256) - grayscale L channel\n",
    "        \n",
    "        # Extract features\n",
    "        low_level_feat = self.low_level_extractor(x)  # (B, 512, 32, 32)\n",
    "        global_feat = self.global_extractor(x)  # (B, 1000)\n",
    "        \n",
    "        # Fuse features\n",
    "        fused = self.fusion(low_level_feat, global_feat)  # (B, 256, 32, 32)\n",
    "        \n",
    "        # Decode to ab channels\n",
    "        ab_pred = self.decoder(fused)  # (B, 2, 256, 256)\n",
    "        \n",
    "        return ab_pred\n",
    "\n",
    "# # Test model instantiation\n",
    "# model = ColorizationModel().to(device)\n",
    "# print(f\"Model created with {sum(p.numel() for p in model.parameters())} parameters\")\n",
    "\n",
    "# # Test forward pass\n",
    "# test_input = torch.randn(2, 1, 256, 256).to(device)\n",
    "# test_output = model(test_input)\n",
    "# print(f\"Input shape: {test_input.shape}\")\n",
    "# print(f\"Output shape: {test_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9051522d",
   "metadata": {},
   "source": [
    "### 4.6 Pretrained Global Feature Extractors\n",
    "\n",
    "We'll integrate pretrained models (VGG, ResNet, EfficientNet) as global feature extractors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7275199",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainedGlobalExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    Global feature extractor using pretrained models.\n",
    "    Supports VGG16, ResNet50, EfficientNet-B0.\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone='vgg16', freeze_backbone=False):\n",
    "        super(PretrainedGlobalExtractor, self).__init__()\n",
    "        \n",
    "        self.backbone_name = backbone\n",
    "        \n",
    "        if backbone == 'vgg16':\n",
    "            # Load pretrained VGG16\n",
    "            vgg = models.vgg16(pretrained=True)\n",
    "            # Modify first conv layer for grayscale input\n",
    "            self.features = nn.Sequential(\n",
    "                nn.Conv2d(1, 64, kernel_size=3, padding=1),  # Modified for grayscale\n",
    "                *list(vgg.features.children())[1:]  # Rest of VGG features\n",
    "            )\n",
    "            feature_dim = 512\n",
    "            \n",
    "        elif backbone == 'resnet50':\n",
    "            # Load pretrained ResNet50\n",
    "            resnet = models.resnet50(pretrained=True)\n",
    "            # Modify first conv layer for grayscale input\n",
    "            self.features = nn.Sequential(\n",
    "                nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "                resnet.bn1,\n",
    "                resnet.relu,\n",
    "                resnet.maxpool,\n",
    "                resnet.layer1,\n",
    "                resnet.layer2,\n",
    "                resnet.layer3,\n",
    "                resnet.layer4\n",
    "            )\n",
    "            feature_dim = 2048\n",
    "            \n",
    "        elif backbone == 'efficientnet_b0':\n",
    "            # Load pretrained EfficientNet-B0\n",
    "            effnet = models.efficientnet_b0(pretrained=True)\n",
    "            # Modify first conv layer for grayscale input\n",
    "            first_conv = effnet.features[0][0]\n",
    "            self.features = nn.Sequential(\n",
    "                nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "                *list(effnet.features.children())[1:]\n",
    "            )\n",
    "            feature_dim = 1280\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown backbone: {backbone}\")\n",
    "        \n",
    "        # Freeze backbone if requested\n",
    "        if freeze_backbone:\n",
    "            for param in self.features.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Pooling and FC layers\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc1 = nn.Linear(feature_dim, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 1000)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input: (B, 1, H, W)\n",
    "        x = self.features(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)  # (B, 1000)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test pretrained extractors\n",
    "# print(\"Testing pretrained global extractors...\")\n",
    "# for backbone in ['vgg16', 'resnet50', 'efficientnet_b0']:\n",
    "#     try:\n",
    "#         extractor = PretrainedGlobalExtractor(backbone=backbone, freeze_backbone=True)\n",
    "#         test_in = torch.randn(2, 1, 256, 256)\n",
    "#         test_out = extractor(test_in)\n",
    "#         print(f\"{backbone}: Input {test_in.shape} -> Output {test_out.shape}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"{backbone}: Error - {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9268e806",
   "metadata": {},
   "source": [
    "## 5. Loss Functions\n",
    "\n",
    "### 5.1 L1 Loss (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4dddb1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L1Loss(nn.Module):\n",
    "    \"\"\"L1 loss for pixel-wise comparison.\"\"\"\n",
    "    def __init__(self):\n",
    "        super(L1Loss, self).__init__()\n",
    "        self.loss = nn.L1Loss()\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        return self.loss(pred, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1c7b40",
   "metadata": {},
   "source": [
    "### 5.2 Perceptual Loss\n",
    "\n",
    "Uses VGG features to compute feature-based loss for more realistic colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "590dee89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptualLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Perceptual loss using VGG16 features.\n",
    "    Compares feature representations rather than pixel values.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        \n",
    "        # Load pretrained VGG16\n",
    "        vgg = models.vgg16(pretrained=True).features\n",
    "        \n",
    "        # Use specific layers for feature extraction\n",
    "        self.slice1 = nn.Sequential(*list(vgg.children())[:4])   # relu1_2\n",
    "        self.slice2 = nn.Sequential(*list(vgg.children())[4:9])  # relu2_2\n",
    "        self.slice3 = nn.Sequential(*list(vgg.children())[9:16]) # relu3_3\n",
    "        \n",
    "        # Freeze VGG parameters\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.mse_loss = nn.MSELoss()\n",
    "    \n",
    "    def forward(self, pred_ab, target_ab, L_channel):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pred_ab: Predicted ab channels (B, 2, H, W)\n",
    "            target_ab: Ground truth ab channels (B, 2, H, W)\n",
    "            L_channel: L channel (B, 1, H, W)\n",
    "        \"\"\"\n",
    "        # Reconstruct LAB images\n",
    "        pred_lab = torch.cat([L_channel * 100, pred_ab * 128], dim=1)\n",
    "        target_lab = torch.cat([L_channel * 100, target_ab * 128], dim=1)\n",
    "        \n",
    "        # Convert LAB to RGB for VGG (VGG expects RGB input)\n",
    "        \n",
    "        # Replicate ab channels to create 3-channel input (approximation)\n",
    "        pred_input = torch.cat([pred_ab, pred_ab[:, :1, :, :]], dim=1)\n",
    "        target_input = torch.cat([target_ab, target_ab[:, :1, :, :]], dim=1)\n",
    "        \n",
    "        # Extract features at different layers\n",
    "        pred_feat1 = self.slice1(pred_input)\n",
    "        target_feat1 = self.slice1(target_input)\n",
    "        \n",
    "        pred_feat2 = self.slice2(pred_feat1)\n",
    "        target_feat2 = self.slice2(target_feat1)\n",
    "        \n",
    "        pred_feat3 = self.slice3(pred_feat2)\n",
    "        target_feat3 = self.slice3(target_feat2)\n",
    "        \n",
    "        # Compute perceptual loss across multiple layers\n",
    "        loss1 = self.mse_loss(pred_feat1, target_feat1)\n",
    "        loss2 = self.mse_loss(pred_feat2, target_feat2)\n",
    "        loss3 = self.mse_loss(pred_feat3, target_feat3)\n",
    "        \n",
    "        return loss1 + loss2 + loss3\n",
    "\n",
    "# # Test perceptual loss\n",
    "# perceptual_loss = PerceptualLoss()\n",
    "# test_L = torch.randn(2, 1, 256, 256)\n",
    "# test_ab_pred = torch.randn(2, 2, 256, 256)\n",
    "# test_ab_target = torch.randn(2, 2, 256, 256)\n",
    "# perc_loss_val = perceptual_loss(test_ab_pred, test_ab_target, test_L)\n",
    "# print(f\"Perceptual loss test: {perc_loss_val.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8fd2d5",
   "metadata": {},
   "source": [
    "## 6. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "465fdd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(pred_ab, target_ab, L_channel):\n",
    "    \"\"\"\n",
    "    Calculate MSE, PSNR, and SSIM metrics for colorization.\n",
    "    \n",
    "    Args:\n",
    "        pred_ab: Predicted ab channels (B, 2, H, W) in range [-1, 1]\n",
    "        target_ab: Target ab channels (B, 2, H, W) in range [-1, 1]\n",
    "        L_channel: L channel (B, 1, H, W) in range [0, 1]\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with MSE, PSNR, and SSIM values\n",
    "    \"\"\"\n",
    "    batch_size = pred_ab.size(0)\n",
    "    \n",
    "    mse_sum = 0.0\n",
    "    psnr_sum = 0.0\n",
    "    ssim_sum = 0.0\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        # Convert to numpy\n",
    "        pred_ab_np = pred_ab[i].cpu().detach().numpy().transpose(1, 2, 0) * 128.0\n",
    "        target_ab_np = target_ab[i].cpu().detach().numpy().transpose(1, 2, 0) * 128.0\n",
    "        L_np = L_channel[i, 0].cpu().detach().numpy() * 100.0\n",
    "        \n",
    "        # Reconstruct LAB images\n",
    "        pred_lab = np.zeros((pred_ab_np.shape[0], pred_ab_np.shape[1], 3), dtype=np.float32)\n",
    "        pred_lab[:, :, 0] = L_np\n",
    "        pred_lab[:, :, 1:] = pred_ab_np\n",
    "        \n",
    "        target_lab = np.zeros((target_ab_np.shape[0], target_ab_np.shape[1], 3), dtype=np.float32)\n",
    "        target_lab[:, :, 0] = L_np\n",
    "        target_lab[:, :, 1:] = target_ab_np\n",
    "        \n",
    "        # Convert to RGB\n",
    "        pred_rgb = color.lab2rgb(pred_lab)\n",
    "        target_rgb = color.lab2rgb(target_lab)\n",
    "        \n",
    "        # Ensure values are in [0, 1] range\n",
    "        pred_rgb = np.clip(pred_rgb, 0, 1)\n",
    "        target_rgb = np.clip(target_rgb, 0, 1)\n",
    "        \n",
    "        # Calculate MSE\n",
    "        mse = np.mean((pred_rgb - target_rgb) ** 2)\n",
    "        mse_sum += mse\n",
    "        \n",
    "        # Calculate PSNR\n",
    "        if mse > 0:\n",
    "            psnr_val = psnr(target_rgb, pred_rgb, data_range=1.0)\n",
    "            psnr_sum += psnr_val\n",
    "        \n",
    "        # Calculate SSIM\n",
    "        ssim_val = ssim(target_rgb, pred_rgb, channel_axis=2, data_range=1.0)\n",
    "        ssim_sum += ssim_val\n",
    "    \n",
    "    return {\n",
    "        'mse': mse_sum / batch_size,\n",
    "        'psnr': psnr_sum / batch_size,\n",
    "        'ssim': ssim_sum / batch_size\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee99f0d8",
   "metadata": {},
   "source": [
    "## 7. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3295a48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device, use_perceptual=False, perceptual_weight=0.1):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    epoch_metrics = {'mse': 0.0, 'psnr': 0.0, 'ssim': 0.0}\n",
    "    \n",
    "    if use_perceptual:\n",
    "        perceptual_loss_fn = PerceptualLoss().to(device)\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc='Training')\n",
    "    for batch_idx, (L, ab, _) in enumerate(pbar):\n",
    "        L, ab = L.to(device), ab.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        ab_pred = model(L)\n",
    "        \n",
    "        # Calculate loss\n",
    "        pixel_loss = criterion(ab_pred, ab)\n",
    "        \n",
    "        if use_perceptual:\n",
    "            perc_loss = perceptual_loss_fn(ab_pred, ab, L)\n",
    "            total_loss = pixel_loss + perceptual_weight * perc_loss\n",
    "        else:\n",
    "            total_loss = pixel_loss\n",
    "        \n",
    "        # Backward pass\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += total_loss.item()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        with torch.no_grad():\n",
    "            metrics = calculate_metrics(ab_pred, ab, L)\n",
    "            for key in metrics:\n",
    "                epoch_metrics[key] += metrics[key]\n",
    "        \n",
    "        pbar.set_postfix({'loss': total_loss.item()})\n",
    "    \n",
    "    # Average over batches\n",
    "    num_batches = len(train_loader)\n",
    "    epoch_loss /= num_batches\n",
    "    for key in epoch_metrics:\n",
    "        epoch_metrics[key] /= num_batches\n",
    "    \n",
    "    return epoch_loss, epoch_metrics\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device, use_perceptual=False, perceptual_weight=0.1):\n",
    "    \"\"\"Validate the model.\"\"\"\n",
    "    model.eval()\n",
    "    epoch_loss = 0.0\n",
    "    epoch_metrics = {'mse': 0.0, 'psnr': 0.0, 'ssim': 0.0}\n",
    "    \n",
    "    if use_perceptual:\n",
    "        perceptual_loss_fn = PerceptualLoss().to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_loader, desc='Validation')\n",
    "        for L, ab, _ in pbar:\n",
    "            L, ab = L.to(device), ab.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            ab_pred = model(L)\n",
    "            \n",
    "            # Calculate loss\n",
    "            pixel_loss = criterion(ab_pred, ab)\n",
    "            \n",
    "            if use_perceptual:\n",
    "                perc_loss = perceptual_loss_fn(ab_pred, ab, L)\n",
    "                total_loss = pixel_loss + perceptual_weight * perc_loss\n",
    "            else:\n",
    "                total_loss = pixel_loss\n",
    "            \n",
    "            epoch_loss += total_loss.item()\n",
    "            \n",
    "            # Calculate metrics\n",
    "            metrics = calculate_metrics(ab_pred, ab, L)\n",
    "            for key in metrics:\n",
    "                epoch_metrics[key] += metrics[key]\n",
    "            \n",
    "            pbar.set_postfix({'loss': total_loss.item()})\n",
    "    \n",
    "    # Average over batches\n",
    "    num_batches = len(val_loader)\n",
    "    epoch_loss /= num_batches\n",
    "    for key in epoch_metrics:\n",
    "        epoch_metrics[key] /= num_batches\n",
    "    \n",
    "    return epoch_loss, epoch_metrics\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs, lr=0.001, \n",
    "                use_perceptual=False, perceptual_weight=0.1, model_name='model'):\n",
    "    \"\"\"\n",
    "    Complete training loop.\n",
    "    \n",
    "    Args:\n",
    "        model: The colorization model\n",
    "        train_loader: Training data loader\n",
    "        val_loader: Validation data loader\n",
    "        num_epochs: Number of epochs to train\n",
    "        lr: Learning rate\n",
    "        use_perceptual: Whether to use perceptual loss\n",
    "        perceptual_weight: Weight for perceptual loss\n",
    "        model_name: Name for saving the model\n",
    "    \"\"\"\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_metrics_history = {'mse': [], 'psnr': [], 'ssim': []}\n",
    "    val_metrics_history = {'mse': [], 'psnr': [], 'ssim': []}\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    print(f\"Training {model_name}...\")\n",
    "    print(f\"Using perceptual loss: {use_perceptual}\")\n",
    "    if use_perceptual:\n",
    "        print(f\"Perceptual loss weight: {perceptual_weight}\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_metrics = train_epoch(model, train_loader, criterion, optimizer, \n",
    "                                                device, use_perceptual, perceptual_weight)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_metrics = validate(model, val_loader, criterion, device, \n",
    "                                        use_perceptual, perceptual_weight)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Record history\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        for key in train_metrics:\n",
    "            train_metrics_history[key].append(train_metrics[key])\n",
    "            val_metrics_history[key].append(val_metrics[key])\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"Train - MSE: {train_metrics['mse']:.4f}, PSNR: {train_metrics['psnr']:.2f}, SSIM: {train_metrics['ssim']:.4f}\")\n",
    "        print(f\"Val   - MSE: {val_metrics['mse']:.4f}, PSNR: {val_metrics['psnr']:.2f}, SSIM: {val_metrics['ssim']:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            model_save_path = os.path.join(MODEL_PATH, f'{model_name}_best.pth')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "            }, model_save_path)\n",
    "            print(f\"Saved best model to {model_save_path} with val_loss: {val_loss:.4f}\")\n",
    "    \n",
    "    history = {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_metrics': train_metrics_history,\n",
    "        'val_metrics': val_metrics_history\n",
    "    }\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d66adb6",
   "metadata": {},
   "source": [
    "## 8. Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcba13ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(history, model_name='Model'):\n",
    "    \"\"\"Plot training and validation curves.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0, 0].plot(history['train_losses'], label='Train Loss')\n",
    "    axes[0, 0].plot(history['val_losses'], label='Val Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title(f'{model_name} - Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    # MSE\n",
    "    axes[0, 1].plot(history['train_metrics']['mse'], label='Train MSE')\n",
    "    axes[0, 1].plot(history['val_metrics']['mse'], label='Val MSE')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('MSE')\n",
    "    axes[0, 1].set_title(f'{model_name} - MSE')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "    \n",
    "    # PSNR\n",
    "    axes[1, 0].plot(history['train_metrics']['psnr'], label='Train PSNR')\n",
    "    axes[1, 0].plot(history['val_metrics']['psnr'], label='Val PSNR')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('PSNR (dB)')\n",
    "    axes[1, 0].set_title(f'{model_name} - PSNR')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True)\n",
    "    \n",
    "    # SSIM\n",
    "    axes[1, 1].plot(history['train_metrics']['ssim'], label='Train SSIM')\n",
    "    axes[1, 1].plot(history['val_metrics']['ssim'], label='Val SSIM')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('SSIM')\n",
    "    axes[1, 1].set_title(f'{model_name} - SSIM')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_results(model, dataset, device, num_samples=5):\n",
    "    \"\"\"\n",
    "    Visualize colorization results.\n",
    "    Shows: Input grayscale, Ground truth color, Predicted color\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(12, 4 * num_samples))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            L, ab_true, _ = dataset[i]\n",
    "            \n",
    "            # Add batch dimension\n",
    "            L_batch = L.unsqueeze(0).to(device)\n",
    "            \n",
    "            # Predict\n",
    "            ab_pred = model(L_batch)\n",
    "            \n",
    "            # Convert to numpy\n",
    "            L_np = (L.squeeze().numpy() * 100.0).astype(np.float32)\n",
    "            ab_true_np = (ab_true.permute(1, 2, 0).numpy() * 128.0).astype(np.float32)\n",
    "            ab_pred_np = (ab_pred[0].cpu().permute(1, 2, 0).numpy() * 128.0).astype(np.float32)\n",
    "            \n",
    "            # Reconstruct LAB images\n",
    "            lab_true = np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.float32)\n",
    "            lab_true[:, :, 0] = L_np\n",
    "            lab_true[:, :, 1:] = ab_true_np\n",
    "            \n",
    "            lab_pred = np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.float32)\n",
    "            lab_pred[:, :, 0] = L_np\n",
    "            lab_pred[:, :, 1:] = ab_pred_np\n",
    "            \n",
    "            # Convert to RGB\n",
    "            rgb_true = color.lab2rgb(lab_true)\n",
    "            rgb_pred = color.lab2rgb(lab_pred)\n",
    "            \n",
    "            # Plot\n",
    "            axes[i, 0].imshow(L_np, cmap='gray')\n",
    "            axes[i, 0].set_title(f'Sample {i+1}: Grayscale Input')\n",
    "            axes[i, 0].axis('off')\n",
    "            \n",
    "            axes[i, 1].imshow(rgb_true)\n",
    "            axes[i, 1].set_title('Ground Truth')\n",
    "            axes[i, 1].axis('off')\n",
    "            \n",
    "            axes[i, 2].imshow(rgb_pred)\n",
    "            axes[i, 2].set_title('Predicted')\n",
    "            axes[i, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def compare_models(models_dict, dataset, device, num_samples=3):\n",
    "    \"\"\"\n",
    "    Compare results from multiple models side by side.\n",
    "    \n",
    "    Args:\n",
    "        models_dict: Dictionary of {model_name: model}\n",
    "        dataset: Test dataset\n",
    "        device: Device to run on\n",
    "        num_samples: Number of samples to compare\n",
    "    \"\"\"\n",
    "    num_models = len(models_dict)\n",
    "    fig, axes = plt.subplots(num_samples, num_models + 2, figsize=(4 * (num_models + 2), 4 * num_samples))\n",
    "    \n",
    "    for model in models_dict.values():\n",
    "        model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            L, ab_true, _ = dataset[i]\n",
    "            L_batch = L.unsqueeze(0).to(device)\n",
    "            \n",
    "            # Convert ground truth\n",
    "            L_np = (L.squeeze().numpy() * 100.0).astype(np.float32)\n",
    "            ab_true_np = (ab_true.permute(1, 2, 0).numpy() * 128.0).astype(np.float32)\n",
    "            \n",
    "            lab_true = np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.float32)\n",
    "            lab_true[:, :, 0] = L_np\n",
    "            lab_true[:, :, 1:] = ab_true_np\n",
    "            rgb_true = color.lab2rgb(lab_true)\n",
    "            \n",
    "            # Plot input and ground truth\n",
    "            axes[i, 0].imshow(L_np, cmap='gray')\n",
    "            axes[i, 0].set_title(f'Sample {i+1}: Input' if i == 0 else 'Input')\n",
    "            axes[i, 0].axis('off')\n",
    "            \n",
    "            axes[i, 1].imshow(rgb_true)\n",
    "            axes[i, 1].set_title('Ground Truth' if i == 0 else '')\n",
    "            axes[i, 1].axis('off')\n",
    "            \n",
    "            # Plot predictions from each model\n",
    "            for j, (model_name, model) in enumerate(models_dict.items()):\n",
    "                ab_pred = model(L_batch)\n",
    "                ab_pred_np = (ab_pred[0].cpu().permute(1, 2, 0).numpy() * 128.0).astype(np.float32)\n",
    "                \n",
    "                lab_pred = np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.float32)\n",
    "                lab_pred[:, :, 0] = L_np\n",
    "                lab_pred[:, :, 1:] = ab_pred_np\n",
    "                rgb_pred = color.lab2rgb(lab_pred)\n",
    "                \n",
    "                axes[i, j + 2].imshow(rgb_pred)\n",
    "                axes[i, j + 2].set_title(model_name if i == 0 else '')\n",
    "                axes[i, j + 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356e5336",
   "metadata": {},
   "source": [
    "## 9. Bonus: PatchGAN Discriminator\n",
    "\n",
    "PatchGAN evaluates small patches of the image to determine if they look realistic, helping produce sharper and more locally consistent colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e0cf321",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchGANDiscriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    PatchGAN discriminator for colorization.\n",
    "    Evaluates 70x70 patches to determine if they look realistic.\n",
    "    \n",
    "    Input: Concatenated L and ab channels (3 channels total)\n",
    "    Output: Patch-wise predictions\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels=3):\n",
    "        super(PatchGANDiscriminator, self).__init__()\n",
    "        \n",
    "        def discriminator_block(in_filters, out_filters, normalization=True):\n",
    "            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
    "            if normalization:\n",
    "                layers.append(nn.BatchNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(input_channels, 64, normalization=False),  # 256 -> 128\n",
    "            *discriminator_block(64, 128),   # 128 -> 64\n",
    "            *discriminator_block(128, 256),  # 64 -> 32\n",
    "            *discriminator_block(256, 512),  # 32 -> 16\n",
    "            nn.Conv2d(512, 1, 4, padding=1)  # 16 -> 15 (output patch size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, img):\n",
    "        # img: (B, 3, 256, 256) - L + ab channels\n",
    "        return self.model(img)\n",
    "\n",
    "\n",
    "def train_with_patchgan(generator, discriminator, train_loader, val_loader, num_epochs, \n",
    "                        lr_g=0.0002, lr_d=0.0002, lambda_l1=100, model_name='gan_model'):\n",
    "    \"\"\"\n",
    "    Train colorization model with PatchGAN discriminator.\n",
    "    \n",
    "    Args:\n",
    "        generator: Colorization model (generator)\n",
    "        discriminator: PatchGAN discriminator\n",
    "        train_loader: Training data loader\n",
    "        val_loader: Validation data loader\n",
    "        num_epochs: Number of epochs\n",
    "        lr_g: Generator learning rate\n",
    "        lr_d: Discriminator learning rate\n",
    "        lambda_l1: Weight for L1 loss\n",
    "        model_name: Name for saving\n",
    "    \"\"\"\n",
    "    criterion_GAN = nn.BCEWithLogitsLoss()\n",
    "    criterion_L1 = nn.L1Loss()\n",
    "    \n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=lr_g, betas=(0.5, 0.999))\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=lr_d, betas=(0.5, 0.999))\n",
    "    \n",
    "    train_g_losses = []\n",
    "    train_d_losses = []\n",
    "    val_losses = []\n",
    "    val_metrics_history = {'mse': [], 'psnr': [], 'ssim': []}\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    print(f\"Training {model_name} with PatchGAN...\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "        \n",
    "        epoch_g_loss = 0.0\n",
    "        epoch_d_loss = 0.0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc='Training')\n",
    "        for L, ab_real, _ in pbar:\n",
    "            batch_size = L.size(0)\n",
    "            L, ab_real = L.to(device), ab_real.to(device)\n",
    "            \n",
    "            # Real and fake labels\n",
    "            real_label = torch.ones((batch_size, 1, 15, 15), device=device)\n",
    "            fake_label = torch.zeros((batch_size, 1, 15, 15), device=device)\n",
    "            \n",
    "            # ============================================\n",
    "            # Train Generator\n",
    "            # ============================================\n",
    "            optimizer_G.zero_grad()\n",
    "            \n",
    "            # Generate fake ab channels\n",
    "            ab_fake = generator(L)\n",
    "            \n",
    "            # Reconstruct LAB images for discriminator\n",
    "            # L in range [0, 1], ab in range [-1, 1]\n",
    "            # Normalize L to [-1, 1] for consistency\n",
    "            L_norm = (L - 0.5) * 2\n",
    "            fake_image = torch.cat([L_norm, ab_fake], dim=1)\n",
    "            \n",
    "            # Adversarial loss\n",
    "            pred_fake = discriminator(fake_image)\n",
    "            loss_GAN = criterion_GAN(pred_fake, real_label)\n",
    "            \n",
    "            # L1 loss\n",
    "            loss_L1 = criterion_L1(ab_fake, ab_real)\n",
    "            \n",
    "            # Total generator loss\n",
    "            loss_G = loss_GAN + lambda_l1 * loss_L1\n",
    "            \n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "            \n",
    "            # ============================================\n",
    "            # Train Discriminator\n",
    "            # ============================================\n",
    "            optimizer_D.zero_grad()\n",
    "            \n",
    "            # Real images\n",
    "            real_image = torch.cat([L_norm, ab_real], dim=1)\n",
    "            pred_real = discriminator(real_image.detach())\n",
    "            loss_D_real = criterion_GAN(pred_real, real_label)\n",
    "            \n",
    "            # Fake images\n",
    "            pred_fake = discriminator(fake_image.detach())\n",
    "            loss_D_fake = criterion_GAN(pred_fake, fake_label)\n",
    "            \n",
    "            # Total discriminator loss\n",
    "            loss_D = (loss_D_real + loss_D_fake) * 0.5\n",
    "            \n",
    "            loss_D.backward()\n",
    "            optimizer_D.step()\n",
    "            \n",
    "            epoch_g_loss += loss_G.item()\n",
    "            epoch_d_loss += loss_D.item()\n",
    "            \n",
    "            pbar.set_postfix({'G_loss': loss_G.item(), 'D_loss': loss_D.item()})\n",
    "        \n",
    "        # Average losses\n",
    "        epoch_g_loss /= len(train_loader)\n",
    "        epoch_d_loss /= len(train_loader)\n",
    "        train_g_losses.append(epoch_g_loss)\n",
    "        train_d_losses.append(epoch_d_loss)\n",
    "        \n",
    "        # Validate\n",
    "        generator.eval()\n",
    "        val_loss = 0.0\n",
    "        val_metrics = {'mse': 0.0, 'psnr': 0.0, 'ssim': 0.0}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for L, ab_real, _ in val_loader:\n",
    "                L, ab_real = L.to(device), ab_real.to(device)\n",
    "                \n",
    "                ab_fake = generator(L)\n",
    "                loss = criterion_L1(ab_fake, ab_real)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                metrics = calculate_metrics(ab_fake, ab_real, L)\n",
    "                for key in metrics:\n",
    "                    val_metrics[key] += metrics[key]\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        for key in val_metrics:\n",
    "            val_metrics[key] /= len(val_loader)\n",
    "        \n",
    "        val_losses.append(val_loss)\n",
    "        for key in val_metrics:\n",
    "            val_metrics_history[key].append(val_metrics[key])\n",
    "        \n",
    "        print(f\"G Loss: {epoch_g_loss:.4f}, D Loss: {epoch_d_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"Val - MSE: {val_metrics['mse']:.4f}, PSNR: {val_metrics['psnr']:.2f}, SSIM: {val_metrics['ssim']:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            model_save_path = os.path.join(MODEL_PATH, f'{model_name}_best.pth')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'generator_state_dict': generator.state_dict(),\n",
    "                'discriminator_state_dict': discriminator.state_dict(),\n",
    "                'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "                'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "            }, model_save_path)\n",
    "            print(f\"Saved best model to {model_save_path} with val_loss: {val_loss:.4f}\")\n",
    "    \n",
    "    history = {\n",
    "        'train_g_losses': train_g_losses,\n",
    "        'train_d_losses': train_d_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'val_metrics': val_metrics_history\n",
    "    }\n",
    "    \n",
    "    return history\n",
    "\n",
    "# # Test PatchGAN discriminator\n",
    "# discriminator = PatchGANDiscriminator(input_channels=3).to(device)\n",
    "# test_img = torch.randn(2, 3, 256, 256).to(device)\n",
    "# test_disc_out = discriminator(test_img)\n",
    "# print(f\"Discriminator test - Input: {test_img.shape}, Output: {test_disc_out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7cc6cb",
   "metadata": {},
   "source": [
    "## 10. Training Experiments\n",
    "\n",
    "Now we'll train different model configurations and compare them:\n",
    "\n",
    "1. **Baseline Model**: Low-level + baseline global extractor + L1 loss\n",
    "2. **Pretrained Backbone**: Low-level + VGG16/ResNet50 + L1 loss  \n",
    "3. **Perceptual Loss**: Best model + perceptual loss\n",
    "4. **PatchGAN (Bonus)**: Best model + PatchGAN discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6752b70a",
   "metadata": {},
   "source": [
    "### 10.1 Train Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8be271de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory allocated: 0.00 GB\n",
      "Training baseline_model...\n",
      "Using perceptual loss: False\n",
      "\n",
      "Epoch 1/30\n",
      "Training baseline_model...\n",
      "Using perceptual loss: False\n",
      "\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  42%|████▏     | 1661/4000 [04:30<06:21,  6.14it/s, loss=0.0566] \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Train baseline model with L1 loss\u001b[39;00m\n\u001b[32m      3\u001b[39m baseline_model = ColorizationModel().to(device)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m baseline_history = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbaseline_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_perceptual\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbaseline_model\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Plot training curves\u001b[39;00m\n\u001b[32m     15\u001b[39m plot_training_curves(baseline_history, \u001b[33m'\u001b[39m\u001b[33mBaseline Model\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 130\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, num_epochs, lr, use_perceptual, perceptual_weight, model_name)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    129\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m train_loss, train_metrics = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m                                        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_perceptual\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperceptual_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[38;5;66;03m# Validate\u001b[39;00m\n\u001b[32m    134\u001b[39m val_loss, val_metrics = validate(model, val_loader, criterion, device, \n\u001b[32m    135\u001b[39m                                 use_perceptual, perceptual_weight)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, train_loader, criterion, optimizer, device, use_perceptual, perceptual_weight)\u001b[39m\n\u001b[32m     29\u001b[39m total_loss.backward()\n\u001b[32m     30\u001b[39m optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m epoch_loss += \u001b[43mtotal_loss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Calculate metrics\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(f\"GPU memory allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "# Train baseline model with L1 loss\n",
    "baseline_model = ColorizationModel().to(device)\n",
    "baseline_history = train_model(\n",
    "    baseline_model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    lr=LEARNING_RATE,\n",
    "    use_perceptual=False,\n",
    "    model_name='baseline_model'\n",
    ")\n",
    "\n",
    "# Plot training curves\n",
    "plot_training_curves(baseline_history, 'Baseline Model')\n",
    "\n",
    "# Visualize results\n",
    "visualize_results(baseline_model, test_dataset, device, num_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef97274f",
   "metadata": {},
   "source": [
    "### 10.2 Train Model with Pretrained Global Extractor (VGG16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf481c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with VGG16 backbone\n",
    "# vgg_extractor = PretrainedGlobalExtractor(backbone='vgg16', freeze_backbone=True).to(device)\n",
    "# vgg_model = ColorizationModel(use_pretrained_global=vgg_extractor).to(device)\n",
    "\n",
    "# vgg_history = train_model(\n",
    "#     vgg_model, \n",
    "#     train_loader, \n",
    "#     val_loader, \n",
    "#     num_epochs=NUM_EPOCHS,\n",
    "#     lr=LEARNING_RATE,\n",
    "#     use_perceptual=False,\n",
    "#     model_name='vgg16_model'\n",
    "# )\n",
    "\n",
    "# # Plot training curves\n",
    "# plot_training_curves(vgg_history, 'VGG16 Model')\n",
    "\n",
    "# # Visualize results\n",
    "# visualize_results(vgg_model, test_dataset, device, num_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42dea80",
   "metadata": {},
   "source": [
    "### 10.3 Train Model with Perceptual Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afafea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with perceptual loss (using best backbone from previous experiments)\n",
    "# perceptual_model = ColorizationModel(use_pretrained_global=vgg_extractor).to(device)\n",
    "\n",
    "# perceptual_history = train_model(\n",
    "#     perceptual_model, \n",
    "#     train_loader, \n",
    "#     val_loader, \n",
    "#     num_epochs=NUM_EPOCHS,\n",
    "#     lr=LEARNING_RATE,\n",
    "#     use_perceptual=True,\n",
    "#     perceptual_weight=0.1,\n",
    "#     model_name='perceptual_model'\n",
    "# )\n",
    "\n",
    "# # Plot training curves\n",
    "# plot_training_curves(perceptual_history, 'Model with Perceptual Loss')\n",
    "\n",
    "# # Visualize results\n",
    "# visualize_results(perceptual_model, test_dataset, device, num_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b61a94f",
   "metadata": {},
   "source": [
    "### 10.4 Train Model with PatchGAN (Bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673d18a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with PatchGAN discriminator\n",
    "# gan_generator = ColorizationModel(use_pretrained_global=vgg_extractor).to(device)\n",
    "# gan_discriminator = PatchGANDiscriminator(input_channels=3).to(device)\n",
    "\n",
    "# gan_history = train_with_patchgan(\n",
    "#     gan_generator,\n",
    "#     gan_discriminator,\n",
    "#     train_loader,\n",
    "#     val_loader,\n",
    "#     num_epochs=NUM_EPOCHS,\n",
    "#     lr_g=0.0002,\n",
    "#     lr_d=0.0002,\n",
    "#     lambda_l1=100,\n",
    "#     model_name='patchgan_model'\n",
    "# )\n",
    "\n",
    "# # Plot training curves\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "# axes[0].plot(gan_history['train_g_losses'], label='Generator Loss')\n",
    "# axes[0].plot(gan_history['train_d_losses'], label='Discriminator Loss')\n",
    "# axes[0].set_xlabel('Epoch')\n",
    "# axes[0].set_ylabel('Loss')\n",
    "# axes[0].set_title('PatchGAN Training Losses')\n",
    "# axes[0].legend()\n",
    "# axes[0].grid(True)\n",
    "\n",
    "# axes[1].plot(gan_history['val_losses'], label='Validation Loss')\n",
    "# axes[1].set_xlabel('Epoch')\n",
    "# axes[1].set_ylabel('Loss')\n",
    "# axes[1].set_title('PatchGAN Validation Loss')\n",
    "# axes[1].legend()\n",
    "# axes[1].grid(True)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Visualize results\n",
    "# visualize_results(gan_generator, test_dataset, device, num_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf8f62a",
   "metadata": {},
   "source": [
    "## 11. Results and Comparison\n",
    "\n",
    "### 11.1 Compare All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c87800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all trained models\n",
    "# models_to_compare = {\n",
    "#     'Baseline': baseline_model,\n",
    "#     'VGG16': vgg_model,\n",
    "#     'Perceptual': perceptual_model,\n",
    "#     'PatchGAN': gan_generator\n",
    "# }\n",
    "\n",
    "# compare_models(models_to_compare, test_dataset, device, num_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71de95be",
   "metadata": {},
   "source": [
    "### 11.2 Quantitative Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d584338b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table of all models\n",
    "# def evaluate_model_on_testset(model, test_loader, device):\n",
    "#     \"\"\"Evaluate model on entire test set.\"\"\"\n",
    "#     model.eval()\n",
    "#     total_metrics = {'mse': 0.0, 'psnr': 0.0, 'ssim': 0.0}\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for L, ab, _ in test_loader:\n",
    "#             L, ab = L.to(device), ab.to(device)\n",
    "#             ab_pred = model(L)\n",
    "            \n",
    "#             metrics = calculate_metrics(ab_pred, ab, L)\n",
    "#             for key in metrics:\n",
    "#                 total_metrics[key] += metrics[key]\n",
    "    \n",
    "#     # Average over batches\n",
    "#     for key in total_metrics:\n",
    "#         total_metrics[key] /= len(test_loader)\n",
    "    \n",
    "#     return total_metrics\n",
    "\n",
    "# # Evaluate all models\n",
    "# results = {}\n",
    "# for model_name, model in models_to_compare.items():\n",
    "#     print(f\"Evaluating {model_name}...\")\n",
    "#     metrics = evaluate_model_on_testset(model, test_loader, device)\n",
    "#     results[model_name] = metrics\n",
    "\n",
    "# # Create comparison table\n",
    "# results_df = pd.DataFrame(results).T\n",
    "# results_df = results_df.round(4)\n",
    "# print(\"\\n\" + \"=\"*60)\n",
    "# print(\"QUANTITATIVE RESULTS SUMMARY\")\n",
    "# print(\"=\"*60)\n",
    "# print(results_df)\n",
    "# print(\"=\"*60)\n",
    "\n",
    "# # Highlight best values\n",
    "# print(\"\\nBest MSE (lower is better):\", results_df['mse'].min(), \"->\", results_df['mse'].idxmin())\n",
    "# print(\"Best PSNR (higher is better):\", results_df['psnr'].max(), \"->\", results_df['psnr'].idxmax())\n",
    "# print(\"Best SSIM (higher is better):\", results_df['ssim'].max(), \"->\", results_df['ssim'].idxmax())\n",
    "\n",
    "# # Visualize metrics comparison\n",
    "# fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# results_df['mse'].plot(kind='bar', ax=axes[0], color='skyblue')\n",
    "# axes[0].set_title('MSE Comparison (Lower is Better)')\n",
    "# axes[0].set_ylabel('MSE')\n",
    "# axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# results_df['psnr'].plot(kind='bar', ax=axes[1], color='lightgreen')\n",
    "# axes[1].set_title('PSNR Comparison (Higher is Better)')\n",
    "# axes[1].set_ylabel('PSNR (dB)')\n",
    "# axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# results_df['ssim'].plot(kind='bar', ax=axes[2], color='lightcoral')\n",
    "# axes[2].set_title('SSIM Comparison (Higher is Better)')\n",
    "# axes[2].set_ylabel('SSIM')\n",
    "# axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e990ba0",
   "metadata": {},
   "source": [
    "### 11.3 Failure Cases and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6ac316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and visualize failure cases (images with lowest SSIM)\n",
    "# def find_failure_cases(model, dataset, device, num_cases=5):\n",
    "#     \"\"\"Find images where the model performs poorly.\"\"\"\n",
    "#     model.eval()\n",
    "    \n",
    "#     ssim_scores = []\n",
    "#     with torch.no_grad():\n",
    "#         for i in range(len(dataset)):\n",
    "#             L, ab, _ = dataset[i]\n",
    "#             L_batch = L.unsqueeze(0).to(device)\n",
    "#             ab_batch = ab.unsqueeze(0).to(device)\n",
    "            \n",
    "#             ab_pred = model(L_batch)\n",
    "#             metrics = calculate_metrics(ab_pred, ab_batch, L_batch)\n",
    "#             ssim_scores.append((i, metrics['ssim']))\n",
    "    \n",
    "#     # Sort by SSIM (ascending - worst first)\n",
    "#     ssim_scores.sort(key=lambda x: x[1])\n",
    "    \n",
    "#     # Get worst cases\n",
    "#     worst_indices = [idx for idx, _ in ssim_scores[:num_cases]]\n",
    "    \n",
    "#     # Visualize\n",
    "#     fig, axes = plt.subplots(num_cases, 3, figsize=(12, 4 * num_cases))\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for plot_idx, data_idx in enumerate(worst_indices):\n",
    "#             L, ab_true, _ = dataset[data_idx]\n",
    "#             L_batch = L.unsqueeze(0).to(device)\n",
    "            \n",
    "#             ab_pred = model(L_batch)\n",
    "            \n",
    "#             # Convert to numpy\n",
    "#             L_np = (L.squeeze().numpy() * 100.0).astype(np.float32)\n",
    "#             ab_true_np = (ab_true.permute(1, 2, 0).numpy() * 128.0).astype(np.float32)\n",
    "#             ab_pred_np = (ab_pred[0].cpu().permute(1, 2, 0).numpy() * 128.0).astype(np.float32)\n",
    "            \n",
    "#             # Reconstruct LAB images\n",
    "#             lab_true = np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.float32)\n",
    "#             lab_true[:, :, 0] = L_np\n",
    "#             lab_true[:, :, 1:] = ab_true_np\n",
    "            \n",
    "#             lab_pred = np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.float32)\n",
    "#             lab_pred[:, :, 0] = L_np\n",
    "#             lab_pred[:, :, 1:] = ab_pred_np\n",
    "            \n",
    "#             # Convert to RGB\n",
    "#             rgb_true = color.lab2rgb(lab_true)\n",
    "#             rgb_pred = color.lab2rgb(lab_pred)\n",
    "            \n",
    "#             # Calculate SSIM for this case\n",
    "#             ssim_val = ssim(rgb_true, rgb_pred, channel_axis=2, data_range=1.0)\n",
    "            \n",
    "#             # Plot\n",
    "#             axes[plot_idx, 0].imshow(L_np, cmap='gray')\n",
    "#             axes[plot_idx, 0].set_title(f'Failure Case {plot_idx+1}: Input')\n",
    "#             axes[plot_idx, 0].axis('off')\n",
    "            \n",
    "#             axes[plot_idx, 1].imshow(rgb_true)\n",
    "#             axes[plot_idx, 1].set_title('Ground Truth')\n",
    "#             axes[plot_idx, 1].axis('off')\n",
    "            \n",
    "#             axes[plot_idx, 2].imshow(rgb_pred)\n",
    "#             axes[plot_idx, 2].set_title(f'Predicted (SSIM: {ssim_val:.3f})')\n",
    "#             axes[plot_idx, 2].axis('off')\n",
    "    \n",
    "#     plt.suptitle('Failure Cases - Worst Colorization Results', fontsize=16, y=1.001)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# # Find failure cases for best model\n",
    "# # find_failure_cases(vgg_model, test_dataset, device, num_cases=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b275fb",
   "metadata": {},
   "source": [
    "## 12. Discussion\n",
    "\n",
    "### 12.1 Model Performance Analysis\n",
    "\n",
    "**Write your analysis here after training. Consider:**\n",
    "\n",
    "1. **Baseline Model Performance:**\n",
    "   - How well did the baseline encoder-decoder work?\n",
    "   - What were the typical MSE, PSNR, and SSIM values?\n",
    "   - Visual quality of colorizations\n",
    "\n",
    "2. **Effect of Pretrained Global Extractors:**\n",
    "   - Which pretrained backbone (VGG16/ResNet50/EfficientNet) performed best?\n",
    "   - Did freezing the backbone help or hurt performance?\n",
    "   - How much improvement over baseline?\n",
    "   - Training time comparison\n",
    "\n",
    "3. **Impact of Perceptual Loss:**\n",
    "   - Did perceptual loss improve visual quality?\n",
    "   - How did metrics compare to L1-only models?\n",
    "   - Were colors more realistic and natural?\n",
    "   - Any trade-offs observed?\n",
    "\n",
    "4. **PatchGAN Discriminator (Bonus):**\n",
    "   - Did adversarial training improve results?\n",
    "   - Were colors sharper and more consistent?\n",
    "   - Training stability and convergence\n",
    "   - Comparison with non-GAN models\n",
    "\n",
    "5. **Computational Cost:**\n",
    "   - Training time for each model configuration\n",
    "   - Memory requirements\n",
    "   - Inference speed\n",
    "   - Practical considerations for deployment\n",
    "\n",
    "### 12.2 Failure Cases Discussion\n",
    "\n",
    "**Discuss common failure patterns:**\n",
    "- Types of images that are hard to colorize (e.g., unusual objects, ambiguous scenes)\n",
    "- Color bleeding or inconsistencies\n",
    "- Grayscale regions that stayed gray\n",
    "- Over-saturation or under-saturation issues\n",
    "\n",
    "### 12.3 Loss Function Comparison\n",
    "\n",
    "**Compare different loss configurations:**\n",
    "- L1 loss only\n",
    "- Perceptual loss only  \n",
    "- L1 + Perceptual loss combined\n",
    "- Effect of perceptual loss weight\n",
    "\n",
    "### 12.4 Architecture Improvements\n",
    "\n",
    "**Discuss architectural modifications you made:**\n",
    "- Skip connections (if added)\n",
    "- Number of layers\n",
    "- Fusion strategy changes\n",
    "- Activation functions\n",
    "- Normalization techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c3e78b",
   "metadata": {},
   "source": [
    "## 13. Conclusion and Future Work\n",
    "\n",
    "### Summary\n",
    "\n",
    "In this assignment, we implemented a Deep Convolutional Neural Network (DCNN) based image colorization system using an encoder-decoder architecture. Key achievements include:\n",
    "\n",
    "1. **Dataset Creation**: Built a dataset of 5000+ images with proper L*a*b* color space conversion\n",
    "2. **Baseline Model**: Implemented the encoder-decoder architecture from Figure 3 with low-level and global feature extractors\n",
    "3. **Pretrained Backbones**: Integrated VGG16/ResNet50/EfficientNet as global feature extractors\n",
    "4. **Loss Functions**: Experimented with L1 loss, perceptual loss, and their combinations\n",
    "5. **Evaluation**: Used MSE, PSNR, and SSIM metrics for quantitative assessment\n",
    "6. **PatchGAN (Bonus)**: Implemented adversarial training with PatchGAN discriminator\n",
    "\n",
    "### Best Performing Model\n",
    "\n",
    "**[Fill in after training]**\n",
    "- Model configuration: ...\n",
    "- Final metrics: MSE=..., PSNR=..., SSIM=...\n",
    "- Why it performed best: ...\n",
    "\n",
    "### Future Improvements\n",
    "\n",
    "1. **Architecture Enhancements:**\n",
    "   - Add U-Net style skip connections for better feature propagation\n",
    "   - Implement attention mechanisms to focus on important regions\n",
    "   - Use dilated convolutions for larger receptive fields\n",
    "\n",
    "2. **Training Improvements:**\n",
    "   - Data augmentation (flips, rotations, color jittering on input)\n",
    "   - Progressive training (start with low resolution, increase gradually)\n",
    "   - Ensemble multiple models for better results\n",
    "\n",
    "3. **Loss Functions:**\n",
    "   - Experiment with other perceptual loss layers\n",
    "   - Add style loss for texture preservation\n",
    "   - Use focal loss for hard examples\n",
    "\n",
    "4. **Dataset:**\n",
    "   - Increase dataset size (10K+ images)\n",
    "   - Include diverse image categories\n",
    "   - Balance color distribution\n",
    "\n",
    "5. **Post-processing:**\n",
    "   - Apply color smoothing filters\n",
    "   - Histogram matching for better color consistency\n",
    "   - Edge-aware filtering\n",
    "\n",
    "### Lessons Learned\n",
    "\n",
    "1. L*a*b* color space is well-suited for colorization tasks\n",
    "2. Pretrained models provide strong features for global understanding\n",
    "3. Perceptual loss helps generate more realistic colors\n",
    "4. Adversarial training can improve local consistency\n",
    "5. Proper evaluation requires both quantitative metrics and visual inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca81ca7",
   "metadata": {},
   "source": [
    "## 14. References\n",
    "\n",
    "1. Zhang, R., Isola, P., & Efros, A. A. (2016). Colorful image colorization. In European conference on computer vision (pp. 649-666). Springer, Cham.\n",
    "\n",
    "2. Iizuka, S., Simo-Serra, E., & Ishikawa, H. (2016). Let there be color! Joint end-to-end learning of global and local image priors for automatic image colorization with simultaneous classification. ACM Transactions on Graphics (ToG), 35(4), 1-11.\n",
    "\n",
    "3. Isola, P., Zhu, J. Y., Zhou, T., & Efros, A. A. (2017). Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1125-1134).\n",
    "\n",
    "4. Simonyan, K., & Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556.\n",
    "\n",
    "5. He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).\n",
    "\n",
    "6. Johnson, J., Alahi, A., & Fei-Fei, L. (2016). Perceptual losses for real-time style transfer and super-resolution. In European conference on computer vision (pp. 694-711). Springer, Cham.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Notebook**\n",
    "\n",
    "### Instructions for Completion:\n",
    "\n",
    "1. **Create Your Dataset**:\n",
    "   - Collect at least 5000 color images\n",
    "   - Run the dataset creation function\n",
    "   - Save to Google Drive and share with instructor\n",
    "\n",
    "2. **Train Models**:\n",
    "   - Uncomment and run training cells\n",
    "   - Start with baseline model\n",
    "   - Then train with pretrained backbones\n",
    "   - Experiment with perceptual loss\n",
    "   - (Optional) Train with PatchGAN\n",
    "\n",
    "3. **Analyze Results**:\n",
    "   - Fill in the discussion sections with your observations\n",
    "   - Create comparison tables and visualizations\n",
    "   - Identify and analyze failure cases\n",
    "\n",
    "4. **Create Demo Video**:\n",
    "   - Record a 5-minute video walking through your notebook\n",
    "   - Show code execution, results, and comparisons\n",
    "   - Upload to Drive/YouTube and add link at the top\n",
    "\n",
    "5. **Submit**:\n",
    "   - Export notebook as .ipynb\n",
    "   - Create ZIP: `b2220356053.zip` containing the notebook\n",
    "   - Submit via Hacettepe Submit system"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
